Integrating Distributed Acoustic Sensing and PINN Frameworks for Enhanced Indoor Sound Source Localization
High Probability Bounds for Cross-Learning Contextual Bandits with Unknown Context Distributions
AutoRegressive Knowledge Base Completion
Lift Your Molecules: Molecular Graph Generation in Latent Euclidean Space
Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the Wasserstein Geometry
Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets
ProtEx: A Retrieval-Augmented Approach for Protein Function Prediction
GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation
Revealing and Mitigating Over-Attention in Knowledge Editing
ANYTEXT2: Visual Text Generation and Editing with Customizable Attributes
A Generalist Hanabi Agent
Exact Certification of (Graph) Neural Networks Against Label Poisoning
Learning through experience:Episodic memory representation for cognitive agents
Rate/Distortion Constrained Model Quantization for Efficient Storage and Inference
Text as Any-Modality for Zero-shot Classification by Consistent Prompt Tuning
Dynamic Taylor Convolutional Neural Network for Few-Shot Point Cloud Semantic Segmentation
GridAgent: A 2D Grid-Based Game Framework And Benchmark For Multimodal Large Language Models
Integrating Planning into Single-Turn Long-Form Text Generation
SepNorm: Generalization of Lion and Normalised Gradient Methods
L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?
Confounder-Free Continual Learning via Recursive Feature Normalization
Evaluating Privacy Risks of Parameter-Efficient Fine-Tuning
Rethinking Memorization in LLMs: On Learning by Rote vs. with Understanding
GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation
SFi-Former: Sparse Flow induced Attention for Graph Transformer
Hyperparameters in Continual Learning: A Reality Check
Fast Salient Factor Concentration (FSFC) Recurrent Neural Network for Text Classification
Deep ECG-Report Interaction Framework for Cross-Modal Representation Learning
Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions
Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models
Can Transformers Reason Logically? A Study in SAT Solving
Unifying Generative and Dense Retrieval for Sequential Recommendation
Transformers trained on proteins can learn to attend to Euclidean distance
What Makes Large Language Models Reason in (Multi-Turn) Code Generation?
LOGO --- Long cOntext aliGnment via efficient preference Optimization
Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization
The Canaryâ€™s Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text
FedTMOS: Efficient One-Shot Federated Learning with Tsetlin Machine
From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency
CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery
Experimental Design for Nonstationary Optimization
Towards Optimal Adapter Placement for Efficient Transfer Learning
Certified Robustness to Clean-label Poisoning Using Diffusion Denoising
FLAG: Clustered Federated Learning Combining Data and Gradient Information in Heterogeneous Settings
Context is Key: A Benchmark for Forecasting with Essential Textual Information
Teach Multimodal LLMs to Comprehend Electrocardiographic Images
Characterizing trainability, expressivity, and generalization of neural architecture with metrics from neural tangent kernel
A deep inverse-mapping model for a flapping robotic wing
Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective
DAG-Jailbreak: Enhancing Black-box Jailbreak Attacks and Defenses through DAG Dependency Analysis
