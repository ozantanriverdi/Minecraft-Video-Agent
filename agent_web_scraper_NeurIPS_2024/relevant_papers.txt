Humanoid Locomotion as Next Token Prediction
TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment
GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing
VLM Agents Generate Their Own Memories: Distilling Experience into Embodied Programs of Thought
Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale
Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs
Detecting Bugs with Substantial Monetary Consequences by LLM and Rule-based Reasoning
Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing
OmniJARVIS: Unified Vision-Language-Action Tokenization Enables Open-World Instruction Following Agents
GAVEL: Generating Games via Evolution and Language Models
Chain of Agents: Large Language Models Collaborating on Long-Context Tasks
Grounding Multimodal Large Language Models in Actions
SELF-DISCOVER: Large Language Models Self-Compose Reasoning Structures
Grokking of Implicit Reasoning in Transformers: A Mechanistic Journey to the Edge of Generalization
Cross-model Control: Improving Multiple Large Language Models in One-time Training
ReplaceAnything3D: Text-Guided Object Replacement in 3D Scenes with Compositional Scene Representations
Do LLMs Build World Representations? Probing Through the Lens of State Abstraction
Star-Agents: Automatic Data Optimization with LLM Agents for Instruction Tuning
BehaviorGPT: Smart Agent Simulation for Autonomous Driving with Next-Patch Prediction
LLM-based Skill Diffusion for Zero-shot Policy Adaptation
InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction
Leveraging Environment Interaction for Automated PDDL Translation and Planning with Large Language Models
Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following
Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
Recursive Introspection: Teaching Language Model Agents How to Self-Improve
OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and Captioning
Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers
DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph
Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search
SpecExec: Massively Parallel Speculative Decoding For Interactive LLM Inference on Consumer Devices
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment
GenRL: Multimodal-foundation world models for generalization in embodied agents
Do's and Don'ts: Learning Desirable Skills with Instruction Videos
VideoTetris: Towards Compositional Text-to-Video Generation
Boosting the Potential of Large Language Models with an Intelligent Information Assistant
Prediction-Powered Ranking of Large Language Models
VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation
Large Language Models Play StarCraft II:Benchmarks and A Chain of Summarization Approach
LaSe-E2V: Towards Language-guided Semantic-aware Event-to-Video Reconstruction
LLaNA: Large Language and NeRF Assistant
Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space
Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in LLMs
Reflective Multi-Agent Collaboration based on Large Language Models
StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving
ALI-Agent: Assessing LLMs'  Alignment with Human Values via Agent-based Evaluation
Make-An-Agent: A Generalizable Policy Network Generator with Behavior-Prompted Diffusion
TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration
When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search
Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context
Discovery of the Hidden World with Large Language Models
Self-playing Adversarial Language Game Enhances LLM Reasoning
SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge
Streaming Long Video Understanding with Large Language Models
SMART: Scalable Multi-agent Real-time Motion Generation via Next-token Prediction
Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks
RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation
AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning
Information Re-Organization Improves Reasoning in Large Language Models
UMB: Understanding Model Behavior for Open-World Object Detection
Can Large Language Model Agents Simulate Human Trust Behavior?
SocialGPT: Prompting LLMs for Social Relation Reasoning via Greedy Segment Optimization
Can large language models explore in-context?
AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning
VLMimic: Vision Language Models are Visual Imitation Learner for Fine-grained Actions
Scene Graph Generation with Role-Playing Large Language Models
Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy
VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks
Compositional 3D-aware Video Generation with LLM Director
Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing
SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation
Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers
AutoGuide: Automated Generation and Selection of Context-Aware Guidelines for Large Language Model Agents
Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents
iVideoGPT: Interactive VideoGPTs are Scalable World Models
SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM
LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Control and Rendering
ChatCam: Empowering Camera Control through Conversational AI
ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models
Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs
PERIA: Perceive, Reason, Imagine, Act via Holistic Language and Vision Planning for Manipulation
